<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>图形学 on wingstone's blog</title><link>/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/</link><description>Recent content in 图形学 on wingstone's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 30 Sep 2020 13:10:15 +0000</lastBuildDate><atom:link href="/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/index.xml" rel="self" type="application/rss+xml"/><item><title>贴图技术——Parallax Mapping（视差贴图）</title><link>/posts/texture-technique-parallax-mappingshi-chai-tie-tu/</link><pubDate>Wed, 30 Sep 2020 13:10:15 +0000</pubDate><guid>/posts/texture-technique-parallax-mappingshi-chai-tie-tu/</guid><description>视差贴图属于位移贴图(Displacement Mapping)技术的一种，它对根据储存在纹理中的几何信息对顶点进行位移或偏移。一般使用位移贴图之前，需要对模型进行细分（细分着色器），然后进行顶点位移；
位移贴图要想有好的效果，需要大量顶点支持，而使用视差贴图即可省去大量的顶点使用；
视差贴图的原理实际上是对采样纹理坐标进行偏移，而偏移的原理根据视角观察高度图的真实过程进行模拟，因此可以模拟出真实的贴图凹凸遮挡关系；
在实际的使用过程中，一般使用 深度图来代替高度图 ，两种互为反相；
Parallax mapping 最原始视差贴图方法就叫Parallax mapping，其大概原理是：在切线空间下，当前采样坐标获得的高度作为V向量偏移的长度，然后偏移后的长度在切平面的投影即为坐标的偏移量
//_ParallaxHeight为一控制参数 uv -= tex2D(_HightMap, uv)*TV.xy*_ParallaxHeight; //V向量为切空间下的向量 在高度图变化比较剧烈的地方，采用这种方法会有很对问题；因此又发展出了Steep Parallax Mapping（陡峭视差贴图）；
Steep Parallax Mapping 对于高度变化剧烈地方，其实很难通过一步就定位到偏移后的位置，陡峭视差贴图方法实际上就是ray matching方法，使用此方法可以更精确的定位到偏移后位置；但由于ray matching算法的性质，在计算量不足的情况下，容易出现分层的痕迹；针对分层问题，后面提出了Parallax Occlusion Mapping(视差遮蔽映射)方法；
该算法首先将深度值进行分层，每一步递进一层；若当前步所获取深度值与步进深度值的差值的符号，与前一步相反，则可定位当前层深度即为偏移深度；分层数量越多，计算量越大，计算结果越精细；
float numLayers = 10; float depthStep = 1.0 / numLayers; float currentDepth = 0.0; float2 uvStep = TV.xy * _ParallaxHeight/ numLayers; float currentDepthMapValue = tex2D(_DepthTex, uv).r; for(int i = 0; i&amp;lt; 10; i++) { if(currentDepth &amp;gt; currentDepthMapValue) break; uv -= uvStep; currentDepthMapValue = tex2D(_DepthTex, uv).</description></item><item><title>NPR之描边</title><link>/posts/npr-zhi-miao-bian/</link><pubDate>Mon, 28 Sep 2020 15:49:37 +0000</pubDate><guid>/posts/npr-zhi-miao-bian/</guid><description>对于非平滑模型，采用背部扩展描边的方式，容易出现断裂问题；
解决方法为：使用高模的平滑法线来进行背面的扩展，可以使用法线贴图计算高模法线；
对于NPR渲染，其实一般也不怎么使用贴图；所以可以将高模的法线bake到顶点色里面；
由于蒙皮网格会实时计算变换后的切空间下的normal、tangent、binormal；因此不能bake local space下的高模法线，除非将local space下的高模法线bake到tangent或binormal进行存储，或者将tangent space下的高模法线bake到顶点色里进行存储；</description></item><item><title>皮肤渲染方法总结</title><link>/posts/pi-fu-xuan-ran-fang-fa-zong-jie/</link><pubDate>Mon, 28 Sep 2020 14:45:56 +0000</pubDate><guid>/posts/pi-fu-xuan-ran-fang-fa-zong-jie/</guid><description> gpugems1：wrap方法，模拟透射，纹理空间blur gpugems3：不同的Specular BRDF，改善模拟透射，纹理空间 Diffusion，屏幕空间Diffusion siggraph2011：pre-integrated skin rendering GPU Pro 2, Part 2. Rendering, Chapter 1. Pre-Intergrated Skin Shading</description></item><item><title>皮肤渲染——Preintegrated Subsurface Scattering</title><link>/posts/pi-fu-xuan-ran-zhi-preintegrated-subsurface-scattering/</link><pubDate>Mon, 28 Sep 2020 12:18:24 +0000</pubDate><guid>/posts/pi-fu-xuan-ran-zhi-preintegrated-subsurface-scattering/</guid><description>预积分皮肤散射主要解决三种皮肤散射情况：
表面弯曲引起的散射（Surface Curvature）； 表面小凸起引起的散射（Small Surface Bumps）； 投影边缘引起的散射（Shadows）； 皮肤背面的透射问题（Translucency）（自己添加的）； Surface Curvature 单纯的wrap并不符合物理，需要通过diffusion profile积分才能获取正确的wrap； 预积分贴图是在球形假设下进行计算的，因此该方法最大的缺陷是模拟表面拓扑复杂的结构时，有很大不合理之处； 幸运的是，当今模型表面大都是平滑的，而不平滑的小的部分都用法线贴图进行假设计算； 积分方程为：
$$ D(\theta, r) = \frac{\int_{-\pi/2}^{\pi/2} {cos(\theta+x)*R(2rsin(x/2))} ,{\rm d}x}{\int_{-\pi/2}^{\pi/2} {R(2rsin(x/2))} ,{\rm d}x} $$
式中R(d)表示Diffusion profile，表示相应距离下的辐射度，d表示距离。该式表示在曲率半径为r的半球下，对应$\theta$角度下，其它所有角度在该角度下的散射强度；对应模型如下图所示：
**图中的$\theta$应该全部用x来表示！**从图中的右侧的模型能够看出，N为我们要求的散射角度，L与N之间的角度为$\theta$，L在$N+x$处的光照强度为$cos(\theta+x)$；N距N+x的距离为2rsin(x/2)，即弦长；N+x在N处的散射可由R(d)计算得出；
图中的Diffusion profile一般使用高斯核叠加进行表示，对于rgb各成分的高斯核表示为： 最终得到一个Diffusion profile在不同$\theta$角度和曲率半径下的积分分布；将$\theta$角度和曲率半径转换为NdoL和1/r，即可得到常用的预积分贴图；如下图所示： 其中曲率的求法可在shader中借助偏导函数计算，即：
float3 dn = fwidth(N); float3 dp = fwidth(P); float c = length(dn)/length(dp); Small Surface Bumps 对于小的凸起以及皱纹，不能使用预积分来进行计算，但是由于可以多采样，因此可以使用预滤波的方式处理法线贴图；
对于specular使用正常的normalmap，对于diffuse的rgb分别使用针对不同Diffusion profile处理后的normalmap，因此需要4张normalmap；
为了效率考虑，可以使用一种高精度无滤波的normalmap，一张低精度预滤波的normalmap分别针对rgb插值来近似相应的预滤波处理；需要2张normalmap；
可以将低精度无滤波的normalmap省略，直接使用模型法线来代替，这样就可以指使用一张高精度无滤波的normalmap进行处理了；
最终的结果为：进行滤波处理的通道，其法线凹凸变弱，反应在视觉上，就是凹凸处该通道有相应的溢出（根本原因是此通道趋于恒定，而其他通道凹凸变换大）；
Shadows Scattering 皮肤出的散射特性在投影边缘会体现出来，具体模拟此现象是很困难的一件事情，但是我们可以通过一些trick来实现；
首先，阴影强度为0或1，表示被遮挡，或不被遮挡；位于两者之间的值，及表示半影区域，及发生散射的区域（前提是使用软阴影）；因此我们可以针对这一点，利用Diffusion profile进行积分，来得到阴影值与散射强度的关系；
积分的过程是针对位置进行积分，需要将阴影值映射到位置上后才能积分，针对不同的软阴影方法，映射函数是不同的；我们用P()表示阴影值与到blur kernel距离的函数，则此映射为$p^-{1}()$；具体的积分方程为： $$ P_S(s,w) = \frac{\int_{-\infty}^{\infty} {P^'(P^{-1}(s)+x)R(x/w)} ,{\rm d}x}{\int_{-\infty}^{\infty} {R(x/w)} ,{\rm d}x} $$ 针对box blur软阴影（PCF）方法，其过程如下： 需要注意的是，半影区域的一部分（宽度到没提及）作为正常的软阴影计算插值，剩下的一部分作为这些软阴影散射产生的影响；P'()即表示新的半影函数分布；</description></item><item><title>GPU编程之DDX、DDY详解</title><link>/posts/gpu-bian-cheng-zhi-ddxddy-xiang-jie/</link><pubDate>Mon, 21 Sep 2020 16:18:43 +0000</pubDate><guid>/posts/gpu-bian-cheng-zhi-ddxddy-xiang-jie/</guid><description>ddx、ddy函数的含义为返回大约目前变量的偏导数；
所谓偏导数，实际上就是分别在x方向和y方向的单位距离下，变量的差值；
在GPU中用数值进行计算的话，就是相邻像素下该变量的差值；
使用细节 对于矢量变量，仍然返回一个矢量，但是矢量中的每个元素都会计算其偏导数；
该函数只能在fragment program profiles中使用，但是并不是所有的硬件都支持；
当该函数使用在条件分支语句中，ddx、ddy的偏导计算并不是完全正确的，因为并不是所有的fragment都运行在同一个分支中；
当变量所依据的参数是基于中心插值的情况下，偏导计算并不是非常精确；
实现细节 其偏导数的实现方式要依据具体的实现方式；典型的实现方式为fragment被光栅化时，会以为2x2的形式形成像素块（称作quad-fragments），偏导计算就在quad-fragments中相邻的fragment中进行差值计算；这是像素着色器上面的最小工作单元 ，在像素着色器中，会将相邻的四个像素作为不可分隔的一组，送入同一个SM内4个不同的Core。
NV shader thread group提供了OpenGL的扩展，可以查询GPU线程、Core、SM、Warp等硬件相关的属性。在处理2x2的像素块时，那些未被图元覆盖的像素着色器线程将被标记为gl_HelperThreadNV = true，它们的结果将被忽略，也不会被存储，但仍然进行着色器的计算，可辅助一些计算，如导数ddx和ddy。
原文：
The variable gl_HelperThreadNV specifies if the current thread is a helper thread. In implementations supporting this extension, fragment shader invocations may be arranged in SIMD thread groups of 2x2 fragments called &amp;ldquo;quad&amp;rdquo;. When a fragment shader instruction is executed on a quad, it&amp;rsquo;s possible that some fragments within the quad will execute the instruction even if they are not covered by the primitive.</description></item><item><title>移动GPU架构</title><link>/posts/yi-dong-gpu-jia-gou/</link><pubDate>Thu, 17 Sep 2020 13:30:09 +0000</pubDate><guid>/posts/yi-dong-gpu-jia-gou/</guid><description>移动GPU架构经常被称之为TBDR（Tiled Based Deferred Rendering），我们这里也以TBDR代称；实际上移动架构有TBR与TBDR两种，为什么都被称之为TBDR，可以看这篇文章；我们这里使用TBDR来指整个移动GPU架构都包含的TBR特点；
移动TBDR架构与桌面IMR架构 IMR架构 IMR（Immediate Mode Rendering）就如字面意思一样，提交的每个渲染命令都会立即开始执行，并且该渲染命令会在整条流水线中执行完毕后才开始执行下一个渲染命令。
IMR的渲染会存在浪费带宽的情况。例如，当两次渲染有前后遮蔽关系时，IMR模式因为两次draw命令都要执行，因此会存在经过Pixel Shader后的Pixel被Depth test抛弃，这样就浪费了Shader Unit运算能力。不过幸运的是，目前几乎所有的IMR架构的GPU都会提供Early Z的判断方式，一般是在Rasterizer里面对图形的遮蔽关系进行判断，如果需要渲染的图形被遮挡住，那么就直接抛弃该图形而不需要执行Pixel Shader。
IMR的另外一个缺点就是其渲染命令在执行需要随时读写frame buffer，depth buffer和stencil buffer，这带来大量的内存带宽消耗，在移动平台上面访问片外内存是最消耗电量和最耗时的操作。
TBDR架构 移动端的硬件在设计最开始想到的最重要的问题就是功耗，功耗意味着发热量，意味着耗电量，意味着芯片大小…所以gpu也是把功耗摆在第一位，然而在gpu的渲染过程中，对功耗影响最大的是带宽；
每渲染一帧图像，对FrameBuffer的访问量是惊人的（各种test，blend，再算上MSAA, overdraw等等），通常gpu的onchip memory（也就是SRAM，或者L1 L2 cache）很小，这么大的FrameBuffer要存储在离gpu相对较远的DRAM（显存）上，可以把gpu想象成你家，SRAM想象成小区便利店，DRAM想象成市中心超市，从gpu对framebuffer的访问就相当于一辆货车大量的在你家和市中心之间往返运输，带宽和发热量之巨大是手机上无法接受的。
TBDR一般的实现策略是对于cpu过来的commandbuffer，只对他们做vetex process，然后对vs产生的结果暂时保存，等待非得刷新整个FrameBuffer的时候，才真正的随这批绘制做光栅化，做tile-based-rendering。什么是非得刷新整个FrameBuffer的时候？比如Swap Back and Front Buffer，glflush，glfinish，glreadpixels，glcopytexiamge，glbitframebuffer，queryingocclusion，unbind the framebuffer。总之是所有gpu觉得不得不把这块FrameData绘制好的时候。
FrameData这个是tbr特有的在gpu绘制时所需的存储数据，在powervr上叫做arguments buffer，在arm上叫做plolygon lists。
于是移动端的gpu想到了一种化整为零的方法，把巨大的FrameBuffer分解成很多小块，使得每个小块可以被离gpu更近的那个SRAM可以容纳，块的多少取决于你的硬件的SRAM的大小。这样gpu可以分批的一块块的在SRAM上访问framebuffer，一整块都访问好了后整体转移回DRAM上。
对比 那么为什么pc不使用tbr，这是因为实际上直接对DRAM上进行读写的速度是最快的，tbdr需要一块块的绘制然后回拷，可以说如果哪一天手机上可以解决带宽产生的功耗问题，或者说sram可以做的足够大了，那么就没有TBDR什么事了。可以简单的认为TBR牺牲了执行效率，但是换来了相对更难解决的带宽功耗。
TBDR的重要特性 关于early-z 因为tbdr有framedata队列，很多gpu会很聪明的尽量筛去不需要绘制的framedata。所以在tbdr上earlyz，或者stencil test这些是非常有益处的。例如你定义了一个stencil，gpu有可能在对framedata处理的过程中就筛掉了那些不能通过stencil的drawcall了。或者通过scissor test可能一整块tile都不需要绘制。
blending和MSAA的效率其实很高，alpha-test效率很低 回头看下tbdr的渲染管线，对于一个tile上所有pixel的绘制都是在on-chip的mem上的，只在最后绘制好了才整体回拷给dram。所以我们通常认为会造成大量带宽的操作，例如blending（对framebuffer的读和写），msaa(增加对framebuffer读取的次数)其实在tbdr上反而是非常快速的。（当然msaa除了会造成framebuffer访问增多，还会带来渲染像素的数量增多，这个是tbr没什么优化的）
alpha-test这个东西，他对depth的写入是不能预先确定的，它必须等到pixel shader执行，这导致了alpha-test之后的那些framedata失去了early–z的机会，也就破坏了TBDR架构中的延迟渲染特性（FrameData必须进行ps处理，不能继续缓存），也就增加了渲染量。
移动TBDR架构与Render Pipeline中TBDR的区别 Render Pipeline中TBDR可以参考这里；RP中的TBDR指的光照渲染流程中，延迟光照的一种；为了减少DC，提高硬件利用效率；这里的延迟主要指：PS阶段光照的计算不立即计算，而是渲染到G-buffer中进行缓存，到最后使用的新的DC来使用G-buffer进行光照的计算（主要用于多光源下的渲染处理）；
移动TBDR架构指的是GPU所采用的的一种渲染架构，是GPU硬件上的延迟渲染流程的硬件实现；而且这里的延迟渲染主要指：VS到PS之间有一个硬件的framedata队列，来延迟PS的处理；
Reference 移动架构浅析 移动设备GPU架构知识汇总 针对移动端TBDR架构GPU特性的渲染优化</description></item><item><title>头发渲染——Kajiya model</title><link>/posts/tou-fa-xuan-ran-zhi-kajiya-model/</link><pubDate>Wed, 16 Sep 2020 12:19:32 +0000</pubDate><guid>/posts/tou-fa-xuan-ran-zhi-kajiya-model/</guid><description>关键点：采用多边形建模，进行深度排序修正渲染顺序（因为半透问题），AO去模拟自阴影，两层高光，采用Tangent向量代替N进行高光计算；
采用多边形建模 头发建模可分为发丝建模（关于发丝建模的渲染看这里）与多边形建模两种，当今游戏界所大量采用的做法也是多边形建模；
多边形建模有更低的几何复杂性，以至于有更高的排序效率；相比之下采用发丝建模需要大约100K-150K的发丝来构建，复杂度高很多； 采用多边形建模可以更加容易的集成到已有的渲染管线中去，基本已有的渲染管线都是处理的多边形模型； 高光计算 主要的高光计算都集成在下面这张PPT上； 可以看出，kajiya计算模型与blin-phong模型比较类似，本质上都是采用pow(NdotH, specularity)来进行的高光计算；但是kajiya模型没有使用多边形几何的法线来作为法线计算，而是采用法线平面的概念来作为法线的代替计算；
虽然几何是多边形，但是仍然将其作为发丝来看待，Tangent向量作为发丝的方向；而发丝的法线应该位于与发丝垂直的平面上，且发丝与此平面的交点作为法线的起点；
法线平面即红色平面所显示的，法线平面中真正的法线，有T向量、H向量所决定的平面，与法线平面的交线来决定，我们将这里决定出来的法线用N1（区别于真正的多边形几何法线）来代替；由于T、H、N1都是单位向量，由几何关系可以得到N1dotH = sin(T,H)，到此，我们就可以使用T、H来进行高光计算了；
模拟真正的头发高光 为了模拟头发真正的高光，还要基于对头发高光的观察进行部分假设，相应的观察假设在这张PPT上； 头发有两层高光； 主高光切变流向朝向发梢； 次高光拥有头发的颜色，且切变流向发根； 次高光带有闪烁效果，即不是很连续； 模拟两层高光比较简单，只需要计算两次高光即可；
如何模拟高光的切变流向，即一个位置偏向发梢，一个偏向发根；因为我们使用模型的T来计算的高光，要想改变高光位置，只能从T下手；AMD提供的方法为，使用N（这里是多边形几何的法线，不是法线平面中的法线N1）对T进行偏移；偏移量可以从贴图中进行采样，计算公式如下：
float ShiftTangent(float3 T, float3 N, float shift) { float3 shiftedT = T + shift * N; return normalize(shiftedT); } 如下图：T&amp;rsquo;与T'&amp;lsquo;是切变后的切向量； T表示发丝的方向，那么当发丝方向发生变化时，N（多边形对应法线）自然而然也同样产生变化，偏移后的N为：
float3 B = cross(N, T); N = cross(T, B); 实质上，N对T的偏移，是模拟头发的起伏，即发丝方向突出多边形平面或凹陷多边形平面；
次高光的闪烁效果模拟比较简单，只需要使用一个噪音纹理与次高光相乘即可；
渲染排序问题 由于头发具有半透效果，必须依据一定的顺序进行渲染才能得到正确的Blend效果；
模型内部排序 由于模型是一簇一簇的，因为只要决定簇之间的排序即可；依据视线观看头发的顺序，可以依照发簇距离头皮的距离进行排序；让靠近头皮的发簇对应的Index buffer排在整个模型Index buffer的前面；这样模型内部的渲染顺序就完全正确了；
修改Index buffer的顺序，可以由模型制作时合并的顺序来决定；也可以由程序进行单独处理；</description></item><item><title>皮肤渲染——Screen Space Separable Subsurface Scattering</title><link>/posts/pi-fu-xuan-ran-zhi-screen-space-separable-subsurface-scattering/</link><pubDate>Wed, 16 Sep 2020 11:31:29 +0000</pubDate><guid>/posts/pi-fu-xuan-ran-zhi-screen-space-separable-subsurface-scattering/</guid><description>总体来说，渲染步骤为：
首先对于不同的皮肤散射颜色，去计算相应的kernel（作为一维的颜色数组，存储该距离下次表面散射贡献），kernel是通过多层高斯曲线去拟合偶极子曲线得到的，因此需要多次高斯叠加计算；kernel的长度大小对应模糊的范围，也对应采样数的大小； 正常渲染皮肤，但要使用MRT，将diffuse成分与specular分离，并使用stencil进行标记； 在相机的BeforeImageEffectsOpaque时，进行皮肤部分的separable blur，并使用stencil test，保证只在皮肤部分进行blur；blur时要使用前面计算出来的kernel以及模型的曲率（借助深度的ddx、ddy计算），依据曲率来调整实际blur的范围； 将MRT进行合并，即blur后的diffuse部分与specular部分进行结合； Reference Separable Subsurface Scattering Advanced Techniques for Realistic Real-Time Skin Rendering</description></item><item><title>图形学学习资源推荐</title><link>/posts/tu-xing-xue-zi-yuan-tui-jian/</link><pubDate>Tue, 15 Sep 2020 16:10:15 +0000</pubDate><guid>/posts/tu-xing-xue-zi-yuan-tui-jian/</guid><description>离线渲染 Physically Based Rendering:From Theory To Implementation：大名鼎鼎的《基于物理的渲染：从理论到实现》，离线学习必看，包含了各种光线追踪中用到的技术！官网还提供了第三版书籍的网页版供免费看； 实时渲染 Real-Time Rendering：实时渲染技术的圣经，涵盖了实时渲染技术的各个方面，建议购买实体书查看；最重要的是记得去官网逛逛，官网包含了大量实时渲染的资料；随着实时ray tracing的发展，官网甚至涵盖了大量ray tracing相关的资料；一级推荐！ GPU Gems1-3部曲：学习GPU编程必备系列，学习如何编写shader来实现各种实时渲染技术； Advances in Real-Time Rendering in 3D Graphics and Games：SIGGRAPH中的课程系列，介绍每一年中关于实时计算机图形学中的高级渲染技术； Raster Tek：一个关于DirectX入门的学习资料，对于新手非常推荐；相比于龙书，这个更接近与实际引擎开发，多个渲染功能都集成到一起； d3dcoder：这是龙书系列的官方网站，含有从DirectX9到DirectX12的教程源代码。 learnopengl：OpenGL的入门教程，必读！</description></item><item><title>Pipeline——Render Pipeline/Path（渲染管线/路径）</title><link>/posts/xuan-ran-guan-xian/</link><pubDate>Mon, 14 Sep 2020 14:51:39 +0000</pubDate><guid>/posts/xuan-ran-guan-xian/</guid><description>Render Path称之为渲染路径更为合适，实际上指渲染一帧所要走的流程，这个流程主要用来处理光照，以及后处理等；常见的有Forward/Deferred Rendering；以及其改版Forward+、Tiled Based Deferred Rendering、Clustered Shading；以及更灵活的Frame Graph（寒霜引擎）、SRP（Unity引擎）；
注意：在render之前，一般还会有一个Application stage，用以在CPU上运行一些必要的前置任务：如碰撞检测、全局加速算法（视锥剔除、遮挡剔除）、物理模拟、动画效果等等；处理完这些后，才能进行高效合理的渲染；
Forward Rendering 总的来说，前向渲染绘制次数为光源个数乘以物体个数，通常在一个pass中处理一个光源；然后多个pass处理多个光源，并在pass中通过blend add相加得到总的光照效果；
DC复杂度为O(num(obj)*num(light))，通常会限制light的数量来减少DC；
更甚者，会将限制光源数量（一般附加光源数量为4），并将所有的光照写进一个Uber Shader，通过传递光照参数来实现；
Z-Prepass避免overdraw问题 具体来说，在实际渲染之前，加入了一个称之为z prepass的流程，这个流程关闭了color buffer的写入，同时pixel shader极为简单或者索性为空，可以非常快速的执行完毕并且获得场景中的z buffer；紧接着，我们再关闭z buffer的写入，改depth test function为equal。这样就只绘制我们所能看到的像素了（当然只针对于不透明问题）；
Deferred Rendering 得益于MRT的支持，我们可以发展处延迟渲染，它的核心技术是 第一阶段 在绘制物体时将光照所需要的的basecolor、normal、smoothness等信息存储于G-buffer中，而不进行真正的光照；待物体绘制完后， 第二阶段 再重新使用G-buffer进行光照的计算（及将光照的计算进行推迟）；
传统的延迟渲染在G-Buffer生成之后，会根据光源的形状（light volume），对每个光源执行一次draw call，如果某个像素被light volume覆盖到了，我们就在该像素的位置执行一次当前光源的lighting计算。
需要注意的是，为了防止同一像素被光源正反面计算两次，我们需要在绘制light volume的时候使用单面渲染，如果摄像机在光源内，则需要开启正面剔除，并且将depth test设置为farOrEqual，如果摄像机在光源之外，则开启背面剔除，并且将depth test设置为nearOrEqual。
DC的复杂度为O(num(obj)+num(light))；num(obj)为前期绘制物体的数量，num(light)为后期光照时光源的数量；在光照渲染时，同样通过blend add来实现多光源效果的叠加；
一种简单但耗费资源的做法：可以直接将所有光源信息传至一个shader，在这一个shader中进行所有光源的计算与累积，由于是在整个屏幕中进行计算，也就导致会产生多余的光照计算（光源照不到区域也进行了计算）；优点是只有O(1)的复杂度进行光照计算；
另外，G-Buffer除了用于直接照明外，还能够被用于一些间接照明的效果，比如SSAO，SSR；也正是G-Buffer概念的提出，使得近十年来越来越多的算法从world space向screen space的演进；
Light Pre-Pass Light Pre-Pass是Deferred Rendering的一个变种，它将整个渲染流程分为三个阶段：
只在G-Buffer中存储Z值和Normal值。对比Deferred Render，少了Diffuse Color， Specular Color以及对应位置的材质索引值。 在FS阶段（对应于普通Deferred Rendering的light volume绘制阶段）利用上面的G-Buffer计算出所必须的light properties，比如Normal*LightDir,LightColor,Specular等light properties。将这些计算出的光照进行blend add并存入LightBuffer（就是用来存储light properties的buffer）。 最后将结果送到forward rendering渲染方式计算最后的光照效果；采用Front to Back的绘制顺序，以及前面的LightBuffer进行光照计算； 可以看到光照相关的light properties已经在第二阶段计算过了，第三阶段更多是光照成分的组合，因此又称之为Light Pre-Pass；总体的DC复杂度为O(num(obj)+num(light)+num(obj))，分别对应第一二三阶段；</description></item><item><title>Pipeline——GPU Graphic Pipeline（图形管线）</title><link>/posts/gpu-graphic-pipeline/</link><pubDate>Sun, 13 Sep 2020 09:14:06 +0000</pubDate><guid>/posts/gpu-graphic-pipeline/</guid><description>管线介绍 所谓管线就是一个流程，针对硬件来说，处理一个图元有一个硬件渲染流程Graphic Pipeline（图形管线）；针对实际应用来说，渲染一帧画面也需要一个渲染流程Render Pipeline/Path（渲染管线/路径）；Graphic Pipeline处于更加低级的渲染层次，是渲染一个物体必走的渲染流程；
GPU Graphic Pipeline 具体的管线流程要看实际的硬件驱动，Direct3D每一个版本都有很大的改动，这里以Direct3D11为例进行介绍；具体的文章参考可以看这里：Direct3D 11 Graphics Pipeline，Direct3D 12 Graphics Pipeline；
Input-Assembler Stage（图元装配阶段） 这一阶段主要进行图元的装配，先从用户填充的缓冲中读取数据，然后将数据装配成图元；此阶段可装配成不同的图元类型（如 line lists, triangle strips, or primitives with adjacency）
Vertex Shader Stage（顶点着色阶段） 一个可编程shader阶段，此阶段主要处理IA阶段输入的顶点，执行每顶点的处理（如变换、蒙皮、变形，顶点光照等）；VS阶段总是处理单一顶点，并输出单一顶点；VS阶段必须处于激活状态，VS必须提供；
Tessellation Stages（细分阶段） 该阶段实际上有三个小阶段来完成图元的细分；通过硬件实现细分，GPU Graphic Pipeline能将低细节的模型转换为高细节模型进行渲染；
Hull-Shader Stage（壳着色阶段） 一个可编程shader阶段，用来生成一个patch（和patch constants），每个patch对应一个输入的patch（quad, triangle, or line）；有点像一个基本的图元类型；
Tessellator Stage 一个固定处理阶段，用来生成简单格式的域，一个域代表一个geometry patch并用来生成更小物体的集合（triangles, points, or lines），通过连接domain sample来实现；
Domain-Shader Stage（域着色阶段） 一个可编程shader阶段，用来计算每个domain sample的顶点的位置，
Geometry Shader Stage（几何着色阶段） 一个可编程shader阶段，该阶段同样以顶点作为输入，以顶点作为输出；但与VS有很大不同；
输入顶点数不一定为一，输入顶点数刚好可以可组成一完整图元（two vertices for lines, three vertices for triangles, or single vertex for point）；并且可以携带邻接的图元顶点数据（an additional two vertices for a line, an additional three for a triangle）； 输出顶点数不一定为一，输出的顶点数目可以形成特定的拓扑结构即可，输出的拓扑结构可选（GS stage output topologies available are: tristrip, linestrip, and pointlist）； Stream-Output Stage（流输出阶段） 该阶段的目的在于能够从不断的GS阶段输出顶点数据，至一个或多个缓存中；</description></item><item><title>Opengl Vertex Interpolation</title><link>/posts/opengl-vertex-interpolation/</link><pubDate>Thu, 10 Sep 2020 14:39:18 +0000</pubDate><guid>/posts/opengl-vertex-interpolation/</guid><description>OpenGL中关于插值问题
OpenGL中关于插值问题 在GPU的光栅化阶段，会针对每个片段进行插值计算；需要注意的是，这个时候进行的插值不能是简单的screen space线性插值，而应该是透视空间矫正的view space线性插值；
OpenGL中默认的是透视插值，也就是在view space空间中的线性插值，可以给变量加no perspective in/out使其插值时不做透视矫正，也就不线性了。如果要自定义插值方式，只能自己把相关参数都传到片元shader里计算了。
注意：透视矫正插值仍是线性的，只不过是screen space下等距的线性插值，转换为view space下的非等距的线性插值，也是view space空间上的线性插值；
透视插值的公式推导 参考《3D游戏与计算机图形学中的数学方法》4.4节
简单概括就是：
对于深度插值：由相似三角形变换可以得到，Z的倒数在光栅化时，恰好是按照线性进行插值的；
对于顶点属性插值：也能推导出： $$ b_3 = \frac{\frac{b_1}{z_1}(1-t)+\frac{b_2}{z_2}t}{\frac{1}{z_1}(1-t)+\frac{1}{z_2}t} $$ 也就是说，顶点属性的插值为$b/z$的线性插值再除以$1/z$的线性插值；
为什么像素插值光照要优于顶点插值光照 是因为顶点插值光照是线性插值（当然指透视矫正后）的，但是对于向量只是进行线性插值是不够的，还应该有个归一化阶段；
像素插值光照可以在PS中进行向量的归一化（特别是法向量），如此就纠正了向量插值的错误问题，然后可进行正确的光照；
其次光照计算过程并不是一个线性的过程；对计算结果进行线性插值，当然就不可能得到正确的光照效果；</description></item><item><title>Monte Carlo Integration</title><link>/posts/monte-carlo-integration/</link><pubDate>Sun, 06 Sep 2020 14:35:52 +0000</pubDate><guid>/posts/monte-carlo-integration/</guid><description>蒙特卡洛积分的简短介绍，以及PBRT中常用的函数分布；
概率部分定义 期望定义 $$ E_p[f(x)] = \int_D {f(x)p(x)} ,{\rm d}x. $$
方差定义 $$ V[f(x)] = E[(f(x)-E[f(x)])^2] $$
蒙特卡洛积分 一维均匀分布下的积分 欲求 $$ F_N = \int_a^b {f(x)} ,{\rm d}x. $$ 的积分；
假如我们有一个均匀随机分布的变量$X_i\in[a,b]$，那么蒙特卡洛积分器的形式为： $$ F_N = \frac{b-a}{N}\sum_{i=1}^N f(X_i). $$
证明为： $$ E[F_N] = E[\frac{b-a}{N}\sum_{i=1}^N f(X_i)]. $$ $$ E[F_N] = \frac{b-a}{N}\sum_{i=1}^N E[f(X_i)]. $$ $$ E[F_N] = \frac{b-a}{N}\sum_{i=1}^N \int_a^b {f(x)p(x)} ,{\rm d}x. $$ $$ E[F_N] = \frac{1}{N}\sum_{i=1}^N \int_a^b {f(x)} ,{\rm d}x. $$ $$ E[F_N] = \int_a^b {f(x)} ,{\rm d}x.</description></item><item><title>Mathematics about camera in graphics（图形学中关于相机的数学）</title><link>/posts/mathematics-about-camera-in-3d-game-engine/</link><pubDate>Thu, 18 Jun 2020 14:34:51 +0000</pubDate><guid>/posts/mathematics-about-camera-in-3d-game-engine/</guid><description>以OpenGL中的右手坐标系为例，介绍引擎中和各种应用中跟相机有关的数学；
实现渲染中的相机 观察矩阵 首先理解观察矩阵的作用，观察矩阵是为了将相机位置和转向不同的情况进行统一，最合适的统一方式就是将相机移动到坐标原点，然后将相机朝向变为-Z轴，这样所有的世界在相机看来就是一致的，便于后续的处理；
实际就是将以(0, 0, 0)为原点的世界坐标系转变为以(cameraPos.x, cameraPos.y, cameraPos.z)为原点的坐标系，当然，这里不能少了旋转；
由于矩阵相乘的顺序影响最后的结果，由于旋转矩阵是相对原点进行旋转的，所以自然而然，我们应该先将相机移至原点（平移矩阵）再进行转向（旋转矩阵）；
在OpenGl中，假设相机坐标为cameraPos，朝向分别为Front，Up，Right，坐标为列向量，则相应的矩阵为： 投影矩阵 关于投影矩阵的推导，看这里;
相机中非线性0-1的深度转为线性0-1深度 设非线性深度值0-1为depth，线性深度值0-1为lineardepth，Zn表示-1到1的NDC深度，Ze表示观察空间下的深度，n表示近裁剪面，f表示远裁剪面； 则几者之间的关系为：
其中由公式2可以看出，线性的0-1范围并不是指近裁剪面对应0、远裁剪面对应1，而是相机位置对应0，远裁剪面对应1；
其中公式3可以在上面投影矩阵那篇文章中看到，跟进上面三式，可以得到depth与lineardepth的关系为：
第一人称视角相机的实现 第一人称视角相机，其实就是FPS类游戏中常用的相机，即相机所看的就是游戏中的人眼所看到的，可以自由的前后左右移动，以及左右上下旋转视角；
第一人称视角相机需要存储一些额外的变量，一个是移动的速度MovementSpeed，二是Pitch，Yaw角度，Pitch表示俯仰角，Yaw表示偏航角；
相机自带的变量为Front，Up，Right，cameraPos，worldUp，worldUp表示世界的正上方向；
相机的移动实现 void ProcessKeyBoard(CEMERA_MOVEMENT direction, float deltaTime) { float offset = MovementSpeed * deltaTime; switch (direction) { case FORWARD: Position += Front * offset; break; case BACKWARD: Position -= Front * offset; break; case LEFT: Position -= Right * offset; break; case RIGHT: Position += Right * offset; break; default: break; } } 相机的旋转实现 void ProcessMouseMovement(float xOffset, float yOffset, bool focusCenter = true, GLboolean constrainPitch = true) { xOffset *= MouseSensitivity; yOffset *= MouseSensitivity; Yaw += xOffset; Pitch += yOffset; if (constrainPitch) { if (Pitch &amp;gt; 89.</description></item></channel></rss>